{"pageProps":{"post":{"attributes":{},"html":"<h1>Context Placement</h1>\n<p>Something else we need to consider, especially as context windows get larger and applications get larger (and chats get longer) is context placement.</p>\n<p><a href=\"https://arxiv.org/pdf/2307.03172\">Lost in the Middle: How Language Models Use Long Contexts</a> analyzed the performance of language models with multi-document questions and also key-value retrievals. Huh?</p>\n<p>Basically, they gave the models a ton of documents and asked it questions about some of the content within the documents, and moved where the document with the relevant information was. Sometimes it was at the beginning, other times in the middle, other times at the end.</p>\n<p>What they found was that even though these models are able to get bigger and bigger context windows and handle more tokens, the LLMs did not make use of information in the long input contexts.</p>\n<p>In fact, performance for the LLMs were highest when the information was placed at the beginning of the context and at the end of the context (though the beginning <em>was</em> better than at the end).</p>\n<p>But here&#39;s where things got really weird.</p>\n<p>Information that was in the middle of many documents not only had worse performance than when the information was at the beginning or the end of the document, but actually the model often <em>performed worse</em> than its performance when predicting the answer with <em>no documents</em>.</p>\n<p>This means that even though these models have bigger and bigger context windows that we are able to use, that means that information we give in context in the middle is sometimes literally getting lost to the model in all the context.</p>\n<p>So this means when we have really important conversations, its really important that we place the most critical information to our context first, and any important additional information last to make best use of the model, and what belongs in the middle is the less important stuff.</p>\n<p>This also means when your chats get really long (or your customer&#39;s chats), context may not only &quot;fall off&quot; if you fill the context window, but context in the middle might also get &quot;lost&quot; to the model.</p>\n<p>Basically, this shows us that we need to start new chats when possible. Keep context small when you can. Position context at the front if its incredibly important, and if something else important comes up, add it to the end.</p>\n","markdown":"# Context Placement\n\nSomething else we need to consider, especially as context windows get larger and applications get larger (and chats get longer) is context placement.\n\n[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172) analyzed the performance of language models with multi-document questions and also key-value retrievals. Huh?\n\nBasically, they gave the models a ton of documents and asked it questions about some of the content within the documents, and moved where the document with the relevant information was. Sometimes it was at the beginning, other times in the middle, other times at the end.\n\nWhat they found was that even though these models are able to get bigger and bigger context windows and handle more tokens, the LLMs did not make use of information in the long input contexts.\n\nIn fact, performance for the LLMs were highest when the information was placed at the beginning of the context and at the end of the context (though the beginning _was_ better than at the end).\n\nBut here's where things got really weird.\n\nInformation that was in the middle of many documents not only had worse performance than when the information was at the beginning or the end of the document, but actually the model often _performed worse_ than its performance when predicting the answer with _no documents_.\n\nThis means that even though these models have bigger and bigger context windows that we are able to use, that means that information we give in context in the middle is sometimes literally getting lost to the model in all the context.\n\nSo this means when we have really important conversations, its really important that we place the most critical information to our context first, and any important additional information last to make best use of the model, and what belongs in the middle is the less important stuff.\n\nThis also means when your chats get really long (or your customer's chats), context may not only \"fall off\" if you fill the context window, but context in the middle might also get \"lost\" to the model.\n\nBasically, this shows us that we need to start new chats when possible. Keep context small when you can. Position context at the front if its incredibly important, and if something else important comes up, add it to the end.\n","slug":"context-placement","title":"Context Placement","section":"Core Prompting Techniques","icon":"info-circle","filePath":"/home/runner/work/practical-prompt-engineering/practical-prompt-engineering/lessons/02-core-prompting-techniques/E-context-placement.md","nextSlug":"/practical-prompt-engineering/lessons/advanced-prompting-techniques/structured-output","prevSlug":"/practical-prompt-engineering/lessons/core-prompting-techniques/few-shot"}},"__N_SSG":true}