{"pageProps":{"post":{"attributes":{},"html":"<h1>LLMs - A Brief Overview</h1>\n<p>Before we can learn any prompting techniques, we need to understand a very basic version and amount of how LLMs work.</p>\n<p>I call this part: <em><strong>LLMs the Easy Parts</strong></em></p>\n<p>LLMs are pattern prediction machines that generate one token at a time (for now, don&#39;t worry about what a token is, we will call them words for now)...so one word at a time. This means that there is no &quot;planning ahead&quot; for LLMs, they are &quot;thinking&quot; when they are typing. They are kind of similar to the autocomplete on your phone, but MUCH better, which we will talk about <em>why</em> shortly.</p>\n<p>One other important thing to know first is that LLMs are non-deterministic.</p>\n<p>Something that is deterministic is a calculator. Every time you input the same thing (2+2) you&#39;re going to get the same output (4). LLMs are not like calculators, they are non-deterministic. This means that even if you and I input the same exact content into the same exact model at the same exact time...we&#39;d still get different answers. How much those answers differ can vary widely! Sometimes they&#39;ll look very similar and maybe be a word or two apart, but other times they&#39;ll be completely different!</p>\n<p>You can try this for yourself to prove this - open 5 different chats with your favorite LLM (Claude, ChatGPT, Gemini, etc) and copy/paste the same prompt into each chat. You&#39;ll see the differences!</p>\n<p>Why do we care though?</p>\n<p>Being non-deterministic is important for us to know, because it means there is some <em>randomness</em> in responses, so we have to account for that and try and minimize that randomness. We also can get inaccuracies in this randomness, and want to minimize those.</p>\n<p>On to what I would call: <em><strong>LLMs the Medium Parts</strong></em></p>\n<p>Back in 2017 (which feels like forever ago!), Google published <a href=\"https://arxiv.org/pdf/1706.03762\">Attention Is All You Need</a> which was groundbreaking for the &quot;Transformer Architecture&quot;. Before transformers, models were a lot like your phone&#39;s autocomplete, they could only track a few nearby words.</p>\n<p>The breakthrough here was the &quot;attention&quot; mechanism that let&#39;s models figure out in a much larger content what actually matters for pattern prediction. Without getting too into the new architecture, just know that before this, autocomplete could track a handful of words. If you think about using your phones autocomplete, it starts out really great, but after adding a few words, the quality of the guesses goes down exponentially.</p>\n<p>Now, with this new architecture, we could have hundreds, thousands of words being paid attention to. Models today have context windows (more on that later too) of even a million or more words (tokens technically).</p>\n<p>And briefly, what I call <em><strong>LLMs the Hard Parts</strong></em></p>\n<p>LLMs are neural networks, so they are inspired by how our brains work. Just like we can strengthen our neuron connections through practicing different things, LLMs adjust their <em>parameters</em> through training.</p>\n<p>Training happens in pre-training and fine-tuning sections, and all you need to understand about that is sometimes when using open source models, we can use the &quot;base&quot; model and that&#39;s the model that has gone through pre-training without fine tuning. The fine-tuning stage makes these LLMs helpful assistants, so when you&#39;re talking to ChatGPT or Claude, you&#39;re getting that question/answer and conversational format that comes from the fine-tuning phase.</p>\n<p>Something really cool to know? The entire model is just a parameter file and a few hundred lines of code for inference. That&#39;s all. The intelligence is in those billions of parameters. Pretty incredible, huh?</p>\n","markdown":"# LLMs - A Brief Overview\n\nBefore we can learn any prompting techniques, we need to understand a very basic version and amount of how LLMs work.\n\nI call this part: ***LLMs the Easy Parts***\n\nLLMs are pattern prediction machines that generate one token at a time (for now, don't worry about what a token is, we will call them words for now)...so one word at a time. This means that there is no \"planning ahead\" for LLMs, they are \"thinking\" when they are typing. They are kind of similar to the autocomplete on your phone, but MUCH better, which we will talk about *why* shortly.\n\nOne other important thing to know first is that LLMs are non-deterministic.\n\nSomething that is deterministic is a calculator. Every time you input the same thing (2+2) you're going to get the same output (4). LLMs are not like calculators, they are non-deterministic. This means that even if you and I input the same exact content into the same exact model at the same exact time...we'd still get different answers. How much those answers differ can vary widely! Sometimes they'll look very similar and maybe be a word or two apart, but other times they'll be completely different!\n\nYou can try this for yourself to prove this - open 5 different chats with your favorite LLM (Claude, ChatGPT, Gemini, etc) and copy/paste the same prompt into each chat. You'll see the differences!\n\nWhy do we care though?\n\nBeing non-deterministic is important for us to know, because it means there is some *randomness* in responses, so we have to account for that and try and minimize that randomness. We also can get inaccuracies in this randomness, and want to minimize those.\n\nOn to what I would call: ***LLMs the Medium Parts***\n\nBack in 2017 (which feels like forever ago!), Google published [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) which was groundbreaking for the \"Transformer Architecture\". Before transformers, models were a lot like your phone's autocomplete, they could only track a few nearby words.\n\nThe breakthrough here was the \"attention\" mechanism that let's models figure out in a much larger content what actually matters for pattern prediction. Without getting too into the new architecture, just know that before this, autocomplete could track a handful of words. If you think about using your phones autocomplete, it starts out really great, but after adding a few words, the quality of the guesses goes down exponentially.\n\nNow, with this new architecture, we could have hundreds, thousands of words being paid attention to. Models today have context windows (more on that later too) of even a million or more words (tokens technically).\n\nAnd briefly, what I call ***LLMs the Hard Parts***\n\nLLMs are neural networks, so they are inspired by how our brains work. Just like we can strengthen our neuron connections through practicing different things, LLMs adjust their *parameters* through training.\n\nTraining happens in pre-training and fine-tuning sections, and all you need to understand about that is sometimes when using open source models, we can use the \"base\" model and that's the model that has gone through pre-training without fine tuning. The fine-tuning stage makes these LLMs helpful assistants, so when you're talking to ChatGPT or Claude, you're getting that question/answer and conversational format that comes from the fine-tuning phase.\n\nSomething really cool to know? The entire model is just a parameter file and a few hundred lines of code for inference. That's all. The intelligence is in those billions of parameters. Pretty incredible, huh?","slug":"introduction-to-LLMs","title":"Introduction to LLMs","section":"Introduction","icon":"info-circle","filePath":"/home/runner/work/practical-prompt-engineering/practical-prompt-engineering/lessons/01-introduction/B-introduction-to-LLMs.md","nextSlug":"/practical-prompt-engineering/lessons/introduction/temperature-top-p-tokens-and-context","prevSlug":"/practical-prompt-engineering/lessons/introduction/what-is-prompt-engineering"}},"__N_SSG":true}