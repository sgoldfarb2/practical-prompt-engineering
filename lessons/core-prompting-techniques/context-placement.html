<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="apple-touch-icon" sizes="180x180" href="/practical-prompt-engineering/images/apple-touch-icon.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/practical-prompt-engineering/images/favicon-32x32.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/practical-prompt-engineering/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/practical-prompt-engineering/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/x-icon" href="/practical-prompt-engineering/images/favicon.ico" data-next-head=""/><title data-next-head="">Context Placement – Practical Prompt Engineering for Developers</title><meta name="description" content="Learn how to use Large Language Models (LLMs) like ChatGPT, Claude, and GitHub Copilot to build applications, generate code, and enhance your development workflow. This course covers prompt engineering techniques, best practices, and real-world examples to help you harness the power of AI in your projects." data-next-head=""/><meta name="keywords" content="Prompt,Engineering,Prompt Engineering,LLM,Large Language Model,AI,Artificial Intelligence,ChatGPT,Claude,Copilot,Cursor" data-next-head=""/><meta name="og:description" content="Learn how to use Large Language Models (LLMs) like ChatGPT, Claude, and GitHub Copilot to build applications, generate code, and enhance your development workflow. This course covers prompt engineering techniques, best practices, and real-world examples to help you harness the power of AI in your projects." data-next-head=""/><meta name="og:title" content="Context Placement – Practical Prompt Engineering for Developers" data-next-head=""/><meta name="og:image" content="/practical-prompt-engineering/images/social-share-cover.jpg" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><link data-next-font="size-adjust" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/practical-prompt-engineering/_next/static/css/494300e045a23fee.css" as="style"/><link rel="stylesheet" href="/practical-prompt-engineering/_next/static/css/494300e045a23fee.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/practical-prompt-engineering/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/practical-prompt-engineering/_next/static/chunks/webpack-cf792cf7e3f82e37.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/framework-b1e5f14688f9ffe6.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/main-bb2903324629cbc1.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/pages/_app-a82b77e4dbdbf3dd.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/pages/lessons/%5Bsection%5D/%5Bslug%5D-98978c15d4082bdc.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/Su_DcT-hcs7Ah5fxh9Obs/_buildManifest.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/Su_DcT-hcs7Ah5fxh9Obs/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="remix-app"><header class="navbar"><h1 class="navbar-brand"><a href="/practical-prompt-engineering">Practical Prompt Engineering for Developers</a></h1><div class="navbar-info"><a href="https://frontendmasters.com/courses/" class="cta-btn">Watch on Frontend Masters</a></div></header><div class="content-container"><div class="main"><div class="lesson-container"><div class="lesson"><div class="lesson-content"><h1>Context Placement</h1>
<p>Something else we need to consider, especially as context windows get larger and applications get larger (and chats get longer) is context placement.</p>
<p><a href="https://arxiv.org/pdf/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a> analyzed the performance of language models with multi-document questions and also key-value retrievals. Huh?</p>
<p>Basically, they gave the models a ton of documents and asked it questions about some of the content within the documents, and moved where the document with the relevant information was. Sometimes it was at the beginning, other times in the middle, other times at the end.</p>
<p>What they found was that even though these models are able to get bigger and bigger context windows and handle more tokens, the LLMs did not make use of information in the long input contexts.</p>
<p>In fact, performance for the LLMs were highest when the information was placed at the beginning of the context and at the end of the context (though the beginning <em>was</em> better than at the end).</p>
<p>But here&#39;s where things got really weird.</p>
<p>Information that was in the middle of many documents not only had worse performance than when the information was at the beginning or the end of the document, but actually the model often <em>performed worse</em> than its performance when predicting the answer with <em>no documents</em>.</p>
<p>This means that even though these models have bigger and bigger context windows that we are able to use, that means that information we give in context in the middle is sometimes literally getting lost to the model in all the context.</p>
<p>So this means when we have really important conversations, its really important that we place the most critical information to our context first, and any important additional information last to make best use of the model, and what belongs in the middle is the less important stuff.</p>
<p>This also means when your chats get really long (or your customer&#39;s chats), context may not only &quot;fall off&quot; if you fill the context window, but context in the middle might also get &quot;lost&quot; to the model.</p>
<p>Basically, this shows us that we need to start new chats when possible. Keep context small when you can. Position context at the front if its incredibly important, and if something else important comes up, add it to the end.</p>
</div><div class="lesson-links"><a href="/practical-prompt-engineering/lessons/core-prompting-techniques/few-shot" class="prev">← Previous</a><a href="/practical-prompt-engineering/lessons/advanced-prompting-techniques/structured-output" class="next">Next →</a></div></div><div class="details-bg"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="154" height="154" viewBox="0 0 154 154"><defs><clipPath id="clip-path"><rect id="Rectangle_2238" data-name="Rectangle 2238" width="154" height="154" transform="translate(9467 350)" fill="#fff" stroke="#707070" stroke-width="1"></rect></clipPath><clipPath id="clip-corner-image-active"><rect width="154" height="154"></rect></clipPath></defs><g id="corner-image-active" clip-path="url(#clip-corner-image-active)"><g id="Corner-image-active-2" data-name="Corner-image-active" transform="translate(-9467 -350)" clip-path="url(#clip-path)"><path id="Subtraction_34" data-name="Subtraction 34" d="M-3857.365,1740.766h0l-7.07-7.07,12.89-12.89v14.142l-5.818,5.818Zm-14.142-14.142h0l-7.071-7.07,27.033-27.033v14.143l-19.96,19.96Zm-14.143-14.143h0l-7.07-7.069,41.175-41.175v14.142Zm-14.142-14.142h0l-7.07-7.069,55.317-55.317v14.142Zm-14.142-14.142h0l-7.07-7.069,69.459-69.459v14.142Zm-14.142-14.142h0l-7.07-7.069,76.739-76.739h6.862v7.28Zm-14.143-14.143h0l-7.07-7.069,62.6-62.6h14.142Zm-14.142-14.142h0l-7.07-7.069,48.454-48.454h14.142Zm-14.142-14.142h0l-7.07-7.069,34.312-34.312h14.142Zm-14.142-14.142h0l-7.07-7.069,20.17-20.17h14.142Zm-14.142-14.142h0l-7.071-7.071,6.027-6.027h14.144l-13.1,13.1Zm367.24-56.114v-.909l.455.455-.453.453Z" transform="translate(13472.546 -1236.766)" fill="var(--corner-fill)"></path></g></g></svg></div></div></div></div><footer class="footer"><ul class="socials"><li class="social"><a href="https://github.com/sgoldfarb2"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-github-social"><rect width="32" height="32"></rect></clipPath></defs><g id="github-social" clip-path="url(#clip-github-social)"><g id="Group_272" data-name="Group 272" transform="translate(13522.5 -6994)"><path id="Subtraction_33" data-name="Subtraction 33" d="M-24967.5,8041a15.9,15.9,0,0,1-11.312-4.688A15.893,15.893,0,0,1-24983.5,8025a15.893,15.893,0,0,1,4.689-11.315A15.894,15.894,0,0,1-24967.5,8009a15.894,15.894,0,0,1,11.313,4.686A15.893,15.893,0,0,1-24951.5,8025a15.893,15.893,0,0,1-4.689,11.313A15.9,15.9,0,0,1-24967.5,8041Zm-3.781-4.571h0v3.918h7.895v-6.665a1.836,1.836,0,0,0-1.2-1.718c5.1-.617,7.467-2.975,7.467-7.424a7.176,7.176,0,0,0-1.637-4.728,6.74,6.74,0,0,0,.275-1.812,4.34,4.34,0,0,0-.52-2.452.574.574,0,0,0-.359-.1c-1.061,0-3.465,1.411-3.936,1.694a16.644,16.644,0,0,0-4.2-.489,16.379,16.379,0,0,0-3.969.445c-.846-.5-2.91-1.649-3.859-1.649a.566.566,0,0,0-.354.095,4.3,4.3,0,0,0-.521,2.452,6.7,6.7,0,0,0,.244,1.718,7.346,7.346,0,0,0-1.6,4.822,7.263,7.263,0,0,0,1.533,4.985c1.193,1.359,3.115,2.165,5.871,2.464a1.826,1.826,0,0,0-1.129,1.693v.5h0l-.006,0a7.121,7.121,0,0,1-2.033.363,2.608,2.608,0,0,1-.965-.158,4.438,4.438,0,0,1-1.836-1.881,2.361,2.361,0,0,0-1.248-1.091,3.472,3.472,0,0,0-1.217-.3.584.584,0,0,0-.545.224.282.282,0,0,0,.027.367,1.875,1.875,0,0,0,.447.307,4.732,4.732,0,0,1,.561.355,10.726,10.726,0,0,1,1.682,2.755c.043.092.078.163.105.217a3.876,3.876,0,0,0,2.42,1.185,6.036,6.036,0,0,0,.607.025c.875,0,1.988-.124,2-.125Z" transform="translate(11461 -1015)" fill="var(--footer-icons)"></path><g id="Ellipse_670" data-name="Ellipse 670" transform="translate(-13522.5 6994)" fill="none" stroke="var(--footer-icons)" stroke-width="1"><circle cx="16" cy="16" r="16" stroke="none"></circle><circle cx="16" cy="16" r="15.5" fill="none"></circle></g></g></g></svg></a></li><li class="social"><a href="https://linkedin.com/in/sabrinagoldfarb"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-linkedin-social"><rect width="32" height="32"></rect></clipPath></defs><g id="linkedin-social" clip-path="url(#clip-linkedin-social)"><g id="Group_270" data-name="Group 270" transform="translate(-86.349 -633.073)"><path id="Path_375" data-name="Path 375" d="M115.789,633.073a2.324,2.324,0,0,1,1.682.676,2.194,2.194,0,0,1,.695,1.627V662.8a2.131,2.131,0,0,1-.695,1.609,2.314,2.314,0,0,1-1.646.659H88.69a2.307,2.307,0,0,1-1.646-.659,2.128,2.128,0,0,1-.695-1.609V635.376a2.19,2.19,0,0,1,.695-1.627,2.322,2.322,0,0,1,1.682-.676h27.063Zm-20.224,9.672a2.561,2.561,0,0,0,0-3.584,2.658,2.658,0,0,0-1.938-.712,2.724,2.724,0,0,0-1.957.712,2.371,2.371,0,0,0-.75,1.792,2.4,2.4,0,0,0,.731,1.792,2.605,2.605,0,0,0,1.9.713h.037A2.7,2.7,0,0,0,95.565,642.745ZM96,645.434H91.213V659.88H96Zm17.3,6.144a7.007,7.007,0,0,0-1.573-4.9,5.68,5.68,0,0,0-6.839-.769,5.663,5.663,0,0,0-1.426,1.573v-2.048H98.674q.036.841,0,7.717v6.728h4.791V651.8a3.592,3.592,0,0,1,.146-1.17,2.913,2.913,0,0,1,.878-1.206,2.429,2.429,0,0,1,1.609-.549,2.108,2.108,0,0,1,1.865.914,4.265,4.265,0,0,1,.549,2.341v7.752H113.3Z" fill="var(--footer-icons)"></path></g></g></svg></a></li><li class="social"><div class="terms"><p>Content Licensed Under CC-BY-NC-4.0</p><p>Code Samples and Exercises Licensed Under Apache 2.0</p><p>Site Designed by<!-- --> <a href="https://www.alexdanielson.com/">Alex Danielson</a></p></div></li></ul><div class="theme-icons"><button aria-label="Activate dark mode" title="Activate dark mode" class="theme-toggle"><svg xmlns="http://www.w3.org/2000/svg" width="36px" height="100%" viewBox="0 -960 960 960" fill="var(--text-footer)" role="img"><title>Dark Mode Icon</title><path d="M480-120q-150 0-255-105T120-480q0-150 105-255t255-105q14 0 27.5 1t26.5 3q-41 29-65.5 75.5T444-660q0 90 63 153t153 63q55 0 101-24.5t75-65.5q2 13 3 26.5t1 27.5q0 150-105 255T480-120Zm0-80q88 0 158-48.5T740-375q-20 5-40 8t-40 3q-123 0-209.5-86.5T364-660q0-20 3-40t8-40q-78 32-126.5 102T200-480q0 116 82 198t198 82Z"></path></svg></button></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"attributes":{},"html":"\u003ch1\u003eContext Placement\u003c/h1\u003e\n\u003cp\u003eSomething else we need to consider, especially as context windows get larger and applications get larger (and chats get longer) is context placement.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2307.03172\"\u003eLost in the Middle: How Language Models Use Long Contexts\u003c/a\u003e analyzed the performance of language models with multi-document questions and also key-value retrievals. Huh?\u003c/p\u003e\n\u003cp\u003eBasically, they gave the models a ton of documents and asked it questions about some of the content within the documents, and moved where the document with the relevant information was. Sometimes it was at the beginning, other times in the middle, other times at the end.\u003c/p\u003e\n\u003cp\u003eWhat they found was that even though these models are able to get bigger and bigger context windows and handle more tokens, the LLMs did not make use of information in the long input contexts.\u003c/p\u003e\n\u003cp\u003eIn fact, performance for the LLMs were highest when the information was placed at the beginning of the context and at the end of the context (though the beginning \u003cem\u003ewas\u003c/em\u003e better than at the end).\u003c/p\u003e\n\u003cp\u003eBut here\u0026#39;s where things got really weird.\u003c/p\u003e\n\u003cp\u003eInformation that was in the middle of many documents not only had worse performance than when the information was at the beginning or the end of the document, but actually the model often \u003cem\u003eperformed worse\u003c/em\u003e than its performance when predicting the answer with \u003cem\u003eno documents\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis means that even though these models have bigger and bigger context windows that we are able to use, that means that information we give in context in the middle is sometimes literally getting lost to the model in all the context.\u003c/p\u003e\n\u003cp\u003eSo this means when we have really important conversations, its really important that we place the most critical information to our context first, and any important additional information last to make best use of the model, and what belongs in the middle is the less important stuff.\u003c/p\u003e\n\u003cp\u003eThis also means when your chats get really long (or your customer\u0026#39;s chats), context may not only \u0026quot;fall off\u0026quot; if you fill the context window, but context in the middle might also get \u0026quot;lost\u0026quot; to the model.\u003c/p\u003e\n\u003cp\u003eBasically, this shows us that we need to start new chats when possible. Keep context small when you can. Position context at the front if its incredibly important, and if something else important comes up, add it to the end.\u003c/p\u003e\n","markdown":"# Context Placement\n\nSomething else we need to consider, especially as context windows get larger and applications get larger (and chats get longer) is context placement.\n\n[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172) analyzed the performance of language models with multi-document questions and also key-value retrievals. Huh?\n\nBasically, they gave the models a ton of documents and asked it questions about some of the content within the documents, and moved where the document with the relevant information was. Sometimes it was at the beginning, other times in the middle, other times at the end.\n\nWhat they found was that even though these models are able to get bigger and bigger context windows and handle more tokens, the LLMs did not make use of information in the long input contexts.\n\nIn fact, performance for the LLMs were highest when the information was placed at the beginning of the context and at the end of the context (though the beginning _was_ better than at the end).\n\nBut here's where things got really weird.\n\nInformation that was in the middle of many documents not only had worse performance than when the information was at the beginning or the end of the document, but actually the model often _performed worse_ than its performance when predicting the answer with _no documents_.\n\nThis means that even though these models have bigger and bigger context windows that we are able to use, that means that information we give in context in the middle is sometimes literally getting lost to the model in all the context.\n\nSo this means when we have really important conversations, its really important that we place the most critical information to our context first, and any important additional information last to make best use of the model, and what belongs in the middle is the less important stuff.\n\nThis also means when your chats get really long (or your customer's chats), context may not only \"fall off\" if you fill the context window, but context in the middle might also get \"lost\" to the model.\n\nBasically, this shows us that we need to start new chats when possible. Keep context small when you can. Position context at the front if its incredibly important, and if something else important comes up, add it to the end.\n","slug":"context-placement","title":"Context Placement","section":"Core Prompting Techniques","icon":"info-circle","filePath":"/home/runner/work/practical-prompt-engineering/practical-prompt-engineering/lessons/02-core-prompting-techniques/E-context-placement.md","nextSlug":"/practical-prompt-engineering/lessons/advanced-prompting-techniques/structured-output","prevSlug":"/practical-prompt-engineering/lessons/core-prompting-techniques/few-shot"}},"__N_SSG":true},"page":"/lessons/[section]/[slug]","query":{"section":"core-prompting-techniques","slug":"context-placement"},"buildId":"Su_DcT-hcs7Ah5fxh9Obs","assetPrefix":"/practical-prompt-engineering","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>