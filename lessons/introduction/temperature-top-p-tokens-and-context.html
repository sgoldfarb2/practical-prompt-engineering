<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="apple-touch-icon" sizes="180x180" href="/practical-prompt-engineering/images/apple-touch-icon.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/practical-prompt-engineering/images/favicon-32x32.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/practical-prompt-engineering/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/practical-prompt-engineering/images/favicon-16x16.png" data-next-head=""/><link rel="icon" type="image/x-icon" href="/practical-prompt-engineering/images/favicon.ico" data-next-head=""/><title data-next-head="">Temperature Top P Tokens and Context – Practical Prompt Engineering for Developers</title><meta name="description" content="Learn how to use Large Language Models (LLMs) like ChatGPT, Claude, and GitHub Copilot to build applications, generate code, and enhance your development workflow. This course covers prompt engineering techniques, best practices, and real-world examples to help you harness the power of AI in your projects." data-next-head=""/><meta name="keywords" content="Prompt,Engineering,Prompt Engineering,LLM,Large Language Model,AI,Artificial Intelligence,ChatGPT,Claude,Copilot,Cursor" data-next-head=""/><meta name="og:description" content="Learn how to use Large Language Models (LLMs) like ChatGPT, Claude, and GitHub Copilot to build applications, generate code, and enhance your development workflow. This course covers prompt engineering techniques, best practices, and real-world examples to help you harness the power of AI in your projects." data-next-head=""/><meta name="og:title" content="Temperature Top P Tokens and Context – Practical Prompt Engineering for Developers" data-next-head=""/><meta name="og:image" content="/practical-prompt-engineering/images/social-share-cover.jpg" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><link data-next-font="size-adjust" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/practical-prompt-engineering/_next/static/css/494300e045a23fee.css" as="style"/><link rel="stylesheet" href="/practical-prompt-engineering/_next/static/css/494300e045a23fee.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/practical-prompt-engineering/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/practical-prompt-engineering/_next/static/chunks/webpack-cf792cf7e3f82e37.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/framework-b1e5f14688f9ffe6.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/main-bb2903324629cbc1.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/pages/_app-a82b77e4dbdbf3dd.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/chunks/pages/lessons/%5Bsection%5D/%5Bslug%5D-98978c15d4082bdc.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/Su_DcT-hcs7Ah5fxh9Obs/_buildManifest.js" defer=""></script><script src="/practical-prompt-engineering/_next/static/Su_DcT-hcs7Ah5fxh9Obs/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="remix-app"><header class="navbar"><h1 class="navbar-brand"><a href="/practical-prompt-engineering">Practical Prompt Engineering for Developers</a></h1><div class="navbar-info"><a href="https://frontendmasters.com/courses/" class="cta-btn">Watch on Frontend Masters</a></div></header><div class="content-container"><div class="main"><div class="lesson-container"><div class="lesson"><div class="lesson-content"><h1>Temperature, Top P, Tokens, and Context Windows</h1>
<p><strong>Temperature</strong>: our way of changing a model&#39;s creativity or randomness</p>
<p>Temperature controls how predictable the AI outputs will be. This is a range from 0 to 2. At temperature 0, the LLM will <em>always</em> pick the most likely next word (remember, they are pattern predictors, so their responses are just picking words based on computer science and statistics).</p>
<p>When we raise the temperature, the AI might pick the 2nd most likely word, or the 100th most likely word. This can make our outputs much more creative, but can also add too much randomness and make them incoherent.</p>
<p>This setting is <em>unavailable</em> to us when we are using ChatGPT or Claude in chat, or in Copilot or Cursor, but when we are building API applications, we have the opportunity to adjust this temperature. This is why we must understand what it is, because if you decide to create an AI application after this, you may need to adjust this setting!</p>
<p>If you are writing an application focused on creativity, you might bump the temperature up to 1.4 (remember 2 is completely random and incoherent). But if you are running a medical AI application, you might dial it way down to .5. So keep this in mind when working with these APIs.</p>
<p><strong>Top P</strong>: Top P <em>sounds similar</em> to temperature, but it does something a bit different.</p>
<p>In every word prediction, the percentage of words that can be predicted add up to 100%. So if I ask the color of the sky, &quot;blue&quot; might be 80% most likely, &quot;gray&quot; 15% most likely, and &quot;orange&quot; 5% most likely. We can see that these three options add up to 100%.</p>
<p>But we can <em>change</em> this setting.</p>
<p>If we are running a business and want to only consider the top 90% of options, we can change our top p to .9, and it will only consider the first 90% of token/word options. What does that mean for our sky example? That &quot;orange&quot; would no longer be considered, because its not in the top 90% most likely next words.</p>
<p>This is another way we can control some of the randomness within our application, and we can use these together to make our application as fine-tuned towards or away from creativity as we want.</p>
<p>Again, this is not something we can edit while just chatting with these models, but in AI applications, we <em>can</em> change these. And its important to understand really how LLMs are genuinely just predicting next tokens for our upcoming prompt techniques.</p>
<p><strong>Tokens</strong>: like words...but not.</p>
<p>Okay, now I can finally stop saying words/tokens and just explain what I mean!</p>
<p>Tokens are to the model what words are to humans. Tokens are roughly .75 words, but not always! Weirdly enough &quot;JavaScript&quot; might be one token but &quot;javascript&quot; might be two. We aren&#39;t here to figure out what is an exact token. All we need to understand about tokens is that they are the words/code/etc. that the model is being input or putting back out to us.</p>
<p><strong>Context Windows</strong>: how many tokens an LLM can &quot;remember&quot; at a time.</p>
<p>So now that we know what a token is, let&#39;s talk about context and context windows.</p>
<p>LLMs have no memory. I know, it seems like they do...so how does this work?</p>
<p>Every time you enter an input to the LLM, it responds, and then that whole chat history is sent to the LLM again. So every time you send a message, you don&#39;t see it, but ALL your past messages are also being sent to the LLM. All your inputs, all the LLMs outputs (within that chat).</p>
<p>So this is how LLMs &quot;remember&quot;. By just...rereading the whole conversation over and over again.</p>
<p>But at some point, the LLM is going to run out of &quot;memory&quot; and that&#39;s the context window - the amount of tokens that the LLM is able to remember.</p>
<p>It&#39;s important to understand this because in these prompting techniques, we will be utilizing context windows and learning about when to open new chats and restart those windows.</p>
<p>Its important to also know that <em>context windows can hit their limit without you knowing</em>.</p>
<p>When you hit these limits, the oldest context (so your first inputs/outputs) drop off first. First in, first out. But the model will not tell you this happened! So if you&#39;re having a conversation and now the model doesn&#39;t remember what you said at first...like maybe...&quot;this is the MOST IMPORTANT STRUCTURE IN MY APP&quot;, and you run past that context window, that context is gone. Lost. Without you knowing. While models now have pretty large context windows, this is still an important concept to understand as we move through our prompting techniques today.</p>
</div><div class="lesson-links"><a href="/practical-prompt-engineering/lessons/introduction/introduction-to-LLMs" class="prev">← Previous</a><a href="/practical-prompt-engineering/lessons/core-prompting-techniques/standard-prompt" class="next">Next →</a></div></div><div class="details-bg"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="154" height="154" viewBox="0 0 154 154"><defs><clipPath id="clip-path"><rect id="Rectangle_2238" data-name="Rectangle 2238" width="154" height="154" transform="translate(9467 350)" fill="#fff" stroke="#707070" stroke-width="1"></rect></clipPath><clipPath id="clip-corner-image-active"><rect width="154" height="154"></rect></clipPath></defs><g id="corner-image-active" clip-path="url(#clip-corner-image-active)"><g id="Corner-image-active-2" data-name="Corner-image-active" transform="translate(-9467 -350)" clip-path="url(#clip-path)"><path id="Subtraction_34" data-name="Subtraction 34" d="M-3857.365,1740.766h0l-7.07-7.07,12.89-12.89v14.142l-5.818,5.818Zm-14.142-14.142h0l-7.071-7.07,27.033-27.033v14.143l-19.96,19.96Zm-14.143-14.143h0l-7.07-7.069,41.175-41.175v14.142Zm-14.142-14.142h0l-7.07-7.069,55.317-55.317v14.142Zm-14.142-14.142h0l-7.07-7.069,69.459-69.459v14.142Zm-14.142-14.142h0l-7.07-7.069,76.739-76.739h6.862v7.28Zm-14.143-14.143h0l-7.07-7.069,62.6-62.6h14.142Zm-14.142-14.142h0l-7.07-7.069,48.454-48.454h14.142Zm-14.142-14.142h0l-7.07-7.069,34.312-34.312h14.142Zm-14.142-14.142h0l-7.07-7.069,20.17-20.17h14.142Zm-14.142-14.142h0l-7.071-7.071,6.027-6.027h14.144l-13.1,13.1Zm367.24-56.114v-.909l.455.455-.453.453Z" transform="translate(13472.546 -1236.766)" fill="var(--corner-fill)"></path></g></g></svg></div></div></div></div><footer class="footer"><ul class="socials"><li class="social"><a href="https://github.com/sgoldfarb2"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-github-social"><rect width="32" height="32"></rect></clipPath></defs><g id="github-social" clip-path="url(#clip-github-social)"><g id="Group_272" data-name="Group 272" transform="translate(13522.5 -6994)"><path id="Subtraction_33" data-name="Subtraction 33" d="M-24967.5,8041a15.9,15.9,0,0,1-11.312-4.688A15.893,15.893,0,0,1-24983.5,8025a15.893,15.893,0,0,1,4.689-11.315A15.894,15.894,0,0,1-24967.5,8009a15.894,15.894,0,0,1,11.313,4.686A15.893,15.893,0,0,1-24951.5,8025a15.893,15.893,0,0,1-4.689,11.313A15.9,15.9,0,0,1-24967.5,8041Zm-3.781-4.571h0v3.918h7.895v-6.665a1.836,1.836,0,0,0-1.2-1.718c5.1-.617,7.467-2.975,7.467-7.424a7.176,7.176,0,0,0-1.637-4.728,6.74,6.74,0,0,0,.275-1.812,4.34,4.34,0,0,0-.52-2.452.574.574,0,0,0-.359-.1c-1.061,0-3.465,1.411-3.936,1.694a16.644,16.644,0,0,0-4.2-.489,16.379,16.379,0,0,0-3.969.445c-.846-.5-2.91-1.649-3.859-1.649a.566.566,0,0,0-.354.095,4.3,4.3,0,0,0-.521,2.452,6.7,6.7,0,0,0,.244,1.718,7.346,7.346,0,0,0-1.6,4.822,7.263,7.263,0,0,0,1.533,4.985c1.193,1.359,3.115,2.165,5.871,2.464a1.826,1.826,0,0,0-1.129,1.693v.5h0l-.006,0a7.121,7.121,0,0,1-2.033.363,2.608,2.608,0,0,1-.965-.158,4.438,4.438,0,0,1-1.836-1.881,2.361,2.361,0,0,0-1.248-1.091,3.472,3.472,0,0,0-1.217-.3.584.584,0,0,0-.545.224.282.282,0,0,0,.027.367,1.875,1.875,0,0,0,.447.307,4.732,4.732,0,0,1,.561.355,10.726,10.726,0,0,1,1.682,2.755c.043.092.078.163.105.217a3.876,3.876,0,0,0,2.42,1.185,6.036,6.036,0,0,0,.607.025c.875,0,1.988-.124,2-.125Z" transform="translate(11461 -1015)" fill="var(--footer-icons)"></path><g id="Ellipse_670" data-name="Ellipse 670" transform="translate(-13522.5 6994)" fill="none" stroke="var(--footer-icons)" stroke-width="1"><circle cx="16" cy="16" r="16" stroke="none"></circle><circle cx="16" cy="16" r="15.5" fill="none"></circle></g></g></g></svg></a></li><li class="social"><a href="https://linkedin.com/in/sabrinagoldfarb"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 32 32"><defs><clipPath id="clip-linkedin-social"><rect width="32" height="32"></rect></clipPath></defs><g id="linkedin-social" clip-path="url(#clip-linkedin-social)"><g id="Group_270" data-name="Group 270" transform="translate(-86.349 -633.073)"><path id="Path_375" data-name="Path 375" d="M115.789,633.073a2.324,2.324,0,0,1,1.682.676,2.194,2.194,0,0,1,.695,1.627V662.8a2.131,2.131,0,0,1-.695,1.609,2.314,2.314,0,0,1-1.646.659H88.69a2.307,2.307,0,0,1-1.646-.659,2.128,2.128,0,0,1-.695-1.609V635.376a2.19,2.19,0,0,1,.695-1.627,2.322,2.322,0,0,1,1.682-.676h27.063Zm-20.224,9.672a2.561,2.561,0,0,0,0-3.584,2.658,2.658,0,0,0-1.938-.712,2.724,2.724,0,0,0-1.957.712,2.371,2.371,0,0,0-.75,1.792,2.4,2.4,0,0,0,.731,1.792,2.605,2.605,0,0,0,1.9.713h.037A2.7,2.7,0,0,0,95.565,642.745ZM96,645.434H91.213V659.88H96Zm17.3,6.144a7.007,7.007,0,0,0-1.573-4.9,5.68,5.68,0,0,0-6.839-.769,5.663,5.663,0,0,0-1.426,1.573v-2.048H98.674q.036.841,0,7.717v6.728h4.791V651.8a3.592,3.592,0,0,1,.146-1.17,2.913,2.913,0,0,1,.878-1.206,2.429,2.429,0,0,1,1.609-.549,2.108,2.108,0,0,1,1.865.914,4.265,4.265,0,0,1,.549,2.341v7.752H113.3Z" fill="var(--footer-icons)"></path></g></g></svg></a></li><li class="social"><div class="terms"><p>Content Licensed Under CC-BY-NC-4.0</p><p>Code Samples and Exercises Licensed Under Apache 2.0</p><p>Site Designed by<!-- --> <a href="https://www.alexdanielson.com/">Alex Danielson</a></p></div></li></ul><div class="theme-icons"><button aria-label="Activate dark mode" title="Activate dark mode" class="theme-toggle"><svg xmlns="http://www.w3.org/2000/svg" width="36px" height="100%" viewBox="0 -960 960 960" fill="var(--text-footer)" role="img"><title>Dark Mode Icon</title><path d="M480-120q-150 0-255-105T120-480q0-150 105-255t255-105q14 0 27.5 1t26.5 3q-41 29-65.5 75.5T444-660q0 90 63 153t153 63q55 0 101-24.5t75-65.5q2 13 3 26.5t1 27.5q0 150-105 255T480-120Zm0-80q88 0 158-48.5T740-375q-20 5-40 8t-40 3q-123 0-209.5-86.5T364-660q0-20 3-40t8-40q-78 32-126.5 102T200-480q0 116 82 198t198 82Z"></path></svg></button></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"attributes":{},"html":"\u003ch1\u003eTemperature, Top P, Tokens, and Context Windows\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTemperature\u003c/strong\u003e: our way of changing a model\u0026#39;s creativity or randomness\u003c/p\u003e\n\u003cp\u003eTemperature controls how predictable the AI outputs will be. This is a range from 0 to 2. At temperature 0, the LLM will \u003cem\u003ealways\u003c/em\u003e pick the most likely next word (remember, they are pattern predictors, so their responses are just picking words based on computer science and statistics).\u003c/p\u003e\n\u003cp\u003eWhen we raise the temperature, the AI might pick the 2nd most likely word, or the 100th most likely word. This can make our outputs much more creative, but can also add too much randomness and make them incoherent.\u003c/p\u003e\n\u003cp\u003eThis setting is \u003cem\u003eunavailable\u003c/em\u003e to us when we are using ChatGPT or Claude in chat, or in Copilot or Cursor, but when we are building API applications, we have the opportunity to adjust this temperature. This is why we must understand what it is, because if you decide to create an AI application after this, you may need to adjust this setting!\u003c/p\u003e\n\u003cp\u003eIf you are writing an application focused on creativity, you might bump the temperature up to 1.4 (remember 2 is completely random and incoherent). But if you are running a medical AI application, you might dial it way down to .5. So keep this in mind when working with these APIs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTop P\u003c/strong\u003e: Top P \u003cem\u003esounds similar\u003c/em\u003e to temperature, but it does something a bit different.\u003c/p\u003e\n\u003cp\u003eIn every word prediction, the percentage of words that can be predicted add up to 100%. So if I ask the color of the sky, \u0026quot;blue\u0026quot; might be 80% most likely, \u0026quot;gray\u0026quot; 15% most likely, and \u0026quot;orange\u0026quot; 5% most likely. We can see that these three options add up to 100%.\u003c/p\u003e\n\u003cp\u003eBut we can \u003cem\u003echange\u003c/em\u003e this setting.\u003c/p\u003e\n\u003cp\u003eIf we are running a business and want to only consider the top 90% of options, we can change our top p to .9, and it will only consider the first 90% of token/word options. What does that mean for our sky example? That \u0026quot;orange\u0026quot; would no longer be considered, because its not in the top 90% most likely next words.\u003c/p\u003e\n\u003cp\u003eThis is another way we can control some of the randomness within our application, and we can use these together to make our application as fine-tuned towards or away from creativity as we want.\u003c/p\u003e\n\u003cp\u003eAgain, this is not something we can edit while just chatting with these models, but in AI applications, we \u003cem\u003ecan\u003c/em\u003e change these. And its important to understand really how LLMs are genuinely just predicting next tokens for our upcoming prompt techniques.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTokens\u003c/strong\u003e: like words...but not.\u003c/p\u003e\n\u003cp\u003eOkay, now I can finally stop saying words/tokens and just explain what I mean!\u003c/p\u003e\n\u003cp\u003eTokens are to the model what words are to humans. Tokens are roughly .75 words, but not always! Weirdly enough \u0026quot;JavaScript\u0026quot; might be one token but \u0026quot;javascript\u0026quot; might be two. We aren\u0026#39;t here to figure out what is an exact token. All we need to understand about tokens is that they are the words/code/etc. that the model is being input or putting back out to us.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eContext Windows\u003c/strong\u003e: how many tokens an LLM can \u0026quot;remember\u0026quot; at a time.\u003c/p\u003e\n\u003cp\u003eSo now that we know what a token is, let\u0026#39;s talk about context and context windows.\u003c/p\u003e\n\u003cp\u003eLLMs have no memory. I know, it seems like they do...so how does this work?\u003c/p\u003e\n\u003cp\u003eEvery time you enter an input to the LLM, it responds, and then that whole chat history is sent to the LLM again. So every time you send a message, you don\u0026#39;t see it, but ALL your past messages are also being sent to the LLM. All your inputs, all the LLMs outputs (within that chat).\u003c/p\u003e\n\u003cp\u003eSo this is how LLMs \u0026quot;remember\u0026quot;. By just...rereading the whole conversation over and over again.\u003c/p\u003e\n\u003cp\u003eBut at some point, the LLM is going to run out of \u0026quot;memory\u0026quot; and that\u0026#39;s the context window - the amount of tokens that the LLM is able to remember.\u003c/p\u003e\n\u003cp\u003eIt\u0026#39;s important to understand this because in these prompting techniques, we will be utilizing context windows and learning about when to open new chats and restart those windows.\u003c/p\u003e\n\u003cp\u003eIts important to also know that \u003cem\u003econtext windows can hit their limit without you knowing\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eWhen you hit these limits, the oldest context (so your first inputs/outputs) drop off first. First in, first out. But the model will not tell you this happened! So if you\u0026#39;re having a conversation and now the model doesn\u0026#39;t remember what you said at first...like maybe...\u0026quot;this is the MOST IMPORTANT STRUCTURE IN MY APP\u0026quot;, and you run past that context window, that context is gone. Lost. Without you knowing. While models now have pretty large context windows, this is still an important concept to understand as we move through our prompting techniques today.\u003c/p\u003e\n","markdown":"# Temperature, Top P, Tokens, and Context Windows\n\n**Temperature**: our way of changing a model's creativity or randomness\n\nTemperature controls how predictable the AI outputs will be. This is a range from 0 to 2. At temperature 0, the LLM will *always* pick the most likely next word (remember, they are pattern predictors, so their responses are just picking words based on computer science and statistics).\n\nWhen we raise the temperature, the AI might pick the 2nd most likely word, or the 100th most likely word. This can make our outputs much more creative, but can also add too much randomness and make them incoherent.\n\nThis setting is *unavailable* to us when we are using ChatGPT or Claude in chat, or in Copilot or Cursor, but when we are building API applications, we have the opportunity to adjust this temperature. This is why we must understand what it is, because if you decide to create an AI application after this, you may need to adjust this setting!\n\nIf you are writing an application focused on creativity, you might bump the temperature up to 1.4 (remember 2 is completely random and incoherent). But if you are running a medical AI application, you might dial it way down to .5. So keep this in mind when working with these APIs.\n\n**Top P**: Top P *sounds similar* to temperature, but it does something a bit different.\n\nIn every word prediction, the percentage of words that can be predicted add up to 100%. So if I ask the color of the sky, \"blue\" might be 80% most likely, \"gray\" 15% most likely, and \"orange\" 5% most likely. We can see that these three options add up to 100%.\n\nBut we can *change* this setting.\n\nIf we are running a business and want to only consider the top 90% of options, we can change our top p to .9, and it will only consider the first 90% of token/word options. What does that mean for our sky example? That \"orange\" would no longer be considered, because its not in the top 90% most likely next words.\n\nThis is another way we can control some of the randomness within our application, and we can use these together to make our application as fine-tuned towards or away from creativity as we want.\n\nAgain, this is not something we can edit while just chatting with these models, but in AI applications, we *can* change these. And its important to understand really how LLMs are genuinely just predicting next tokens for our upcoming prompt techniques.\n\n**Tokens**: like words...but not.\n\nOkay, now I can finally stop saying words/tokens and just explain what I mean!\n\nTokens are to the model what words are to humans. Tokens are roughly .75 words, but not always! Weirdly enough \"JavaScript\" might be one token but \"javascript\" might be two. We aren't here to figure out what is an exact token. All we need to understand about tokens is that they are the words/code/etc. that the model is being input or putting back out to us.\n\n**Context Windows**: how many tokens an LLM can \"remember\" at a time.\n\nSo now that we know what a token is, let's talk about context and context windows.\n\nLLMs have no memory. I know, it seems like they do...so how does this work?\n\nEvery time you enter an input to the LLM, it responds, and then that whole chat history is sent to the LLM again. So every time you send a message, you don't see it, but ALL your past messages are also being sent to the LLM. All your inputs, all the LLMs outputs (within that chat).\n\nSo this is how LLMs \"remember\". By just...rereading the whole conversation over and over again.\n\nBut at some point, the LLM is going to run out of \"memory\" and that's the context window - the amount of tokens that the LLM is able to remember.\n\nIt's important to understand this because in these prompting techniques, we will be utilizing context windows and learning about when to open new chats and restart those windows.\n\nIts important to also know that *context windows can hit their limit without you knowing*.\n\nWhen you hit these limits, the oldest context (so your first inputs/outputs) drop off first. First in, first out. But the model will not tell you this happened! So if you're having a conversation and now the model doesn't remember what you said at first...like maybe...\"this is the MOST IMPORTANT STRUCTURE IN MY APP\", and you run past that context window, that context is gone. Lost. Without you knowing. While models now have pretty large context windows, this is still an important concept to understand as we move through our prompting techniques today.","slug":"temperature-top-p-tokens-and-context","title":"Temperature Top P Tokens and Context","section":"Introduction","icon":"info-circle","filePath":"/home/runner/work/practical-prompt-engineering/practical-prompt-engineering/lessons/01-introduction/C-temperature-top-p-tokens-and-context.md","nextSlug":"/practical-prompt-engineering/lessons/core-prompting-techniques/standard-prompt","prevSlug":"/practical-prompt-engineering/lessons/introduction/introduction-to-LLMs"}},"__N_SSG":true},"page":"/lessons/[section]/[slug]","query":{"section":"introduction","slug":"temperature-top-p-tokens-and-context"},"buildId":"Su_DcT-hcs7Ah5fxh9Obs","assetPrefix":"/practical-prompt-engineering","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>